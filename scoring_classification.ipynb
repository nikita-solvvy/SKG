{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This script generates deep learning training data\n",
    "# Author: Nikita Mishra\n",
    "# run: python Deep_learning_training_data.py\n",
    "\n",
    "# TODO: remove repeated solutions and change start and end position accordingly\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import api_client\n",
    "from api_client import *\n",
    "import random\n",
    "from gensim import models\n",
    "\n",
    "\n",
    "#api_client = SourceFileLoader(\"module.name\", \"../api_client.py\").load_module()\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "SOLUTIONS_FILE = ''.join([DATA_DIR, 'solutions_org'])\n",
    "QUERIES_FILE   = ''.join([DATA_DIR, 'queries_org'])\n",
    "CAUCUSES_FILE  = ''.join([DATA_DIR, 'caucuses_org'])\n",
    "\n",
    "MAX_OUTPUT = 1000000\n",
    "\n",
    "MAX_LENGTH = 500\n",
    "orgs_api = ApiClient('http://localhost:3002', '/v1/orgs')\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return\n",
    "\n",
    "def read(filename):\n",
    "    print('Loading from file ', filename, '...')\n",
    "    with open(filename) as data_file:\n",
    "        return data_file.read().splitlines()\n",
    "    \n",
    "def read_padded(filename, MAX_LENGTH):\n",
    "    print('Loading from file ', filename, '... and adding padding.')     \n",
    "    return [pad(item, MAX_LENGTH) for item in read(filename) ]\n",
    "\n",
    "def pad(sentence,maxlength):\n",
    "    LEN = len(sentence.split(' '))\n",
    "    if LEN>=maxlength:\n",
    "        return sentence[:maxlength]\n",
    "    else:\n",
    "        return ' '.join([sentence]+['BUFFER_PAD']*(maxlength-LEN))\n",
    "    \n",
    "def length_distribution( solutions_caucus):\n",
    "    return [len(u.split(' ')) for u in solutions_caucus]\n",
    "\n",
    "\n",
    "def print_dist(x):\n",
    "    print('min:',min(x),'mean:',sum(x)/len(x),'max:',max(x))\n",
    "    \n",
    "def classify(LOGITS_positive, LOGITS_negative):\n",
    "    left_point = min(LOGITS_positive+ LOGITS_negative)\n",
    "    right_point = max(LOGITS_positive+ LOGITS_negative)\n",
    "    errors=[]\n",
    "    num_steps = 100\n",
    "    gap = int(np.ceil(left_point-right_point/(num_steps - 1)))\n",
    "    if gap <=0:\n",
    "        print('Left should be smaller than right')\n",
    "    for threshold in np.arange(left_point, right_point, gap):\n",
    "        error_positive = sum(i > threshold for i in LOGITS_positive) \n",
    "        error_negative = sum(i < threshold for i in LOGITS_negative) \n",
    "        error = error_positive + error_negative\n",
    "        errors.append(error)\n",
    "        #print(threshold, error, error_positive, error_negative)\n",
    "    print('--Summary')\n",
    "    print('Positive scores_distribution:')\n",
    "    print_dist(LOGITS_positive)\n",
    "    print('Negative scores_distribution:')\n",
    "    print_dist(LOGITS_negative)\n",
    "    print('Num errors:', min(errors), ', Percent errors:',min(errors)/(len(LOGITS_negative)+len(LOGITS_positive)))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file  /Users/nikita/nmt_data/Upwork/scoring_results/chunked_caucus_positive_sources.txt ...\n",
      "Loading from file  /Users/nikita/nmt_data/Upwork/scoring_results/chunked_caucus_negative_sources.txt ...\n"
     ]
    }
   ],
   "source": [
    "# read caucus positive negative\n",
    "\n",
    "caucus_positive = read('/Users/nikita/nmt_data/Upwork/scoring_results/chunked_caucus_positive_sources.txt')\n",
    "caucus_negative = read('/Users/nikita/nmt_data/Upwork/scoring_results/chunked_caucus_negative_sources.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caucus_positive_scores = [float(item.split(\" \")[2]) for item in caucus_positive]\n",
    "caucus_negative_scores = [float(item.split(\" \")[2]) for item in caucus_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Summary\n",
      "Positive scores_distribution:\n",
      "min: 50.9601 mean: 416.2241589001449 max: 2514.77\n",
      "Negative scores_distribution:\n",
      "min: 50.3164 mean: 434.5955019536907 max: 4764.79\n",
      "Num errors: 1287 , Percent errors: 0.465629522431\n"
     ]
    }
   ],
   "source": [
    "errors = classify(caucus_positive_scores, caucus_negative_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bagging\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "import math\n",
    "def classify_sample(LOGITS_positive, LOGITS_negative):    \n",
    "        ids = random.sample(range(len(LOGITS_positive)),  math.ceil(len(LOGITS_positive)*0.7) ) \n",
    "        LOGITS_positive = [LOGITS_positive[item] for item in ids ]\n",
    "        LOGITS_negative = [LOGITS_negative[item] for item in ids ]\n",
    "\n",
    "        left_point = min(LOGITS_positive+ LOGITS_negative)\n",
    "        right_point = max(LOGITS_positive+ LOGITS_negative)\n",
    "        errors=[]\n",
    "        num_steps = 100\n",
    "        gap = int(np.ceil(abs(left_point-right_point)/(num_steps - 1)))\n",
    "        if gap <=0:\n",
    "            print('Left should be smaller than right')\n",
    "            print('Left:', left_point, ', Right:', right_point)\n",
    "            return\n",
    "        for threshold in np.arange(left_point, right_point, gap):\n",
    "            error_positive = sum(i > threshold for i in LOGITS_positive) \n",
    "            error_negative = sum(i < threshold for i in LOGITS_negative) \n",
    "            error = error_positive + error_negative\n",
    "            errors.append(error)\n",
    "            #print(threshold, error, error_positive, error_negative)\n",
    "        return min(errors)/(len(LOGITS_negative)+len(LOGITS_positive))\n",
    "def classify_bagged(LOGITS_positive, LOGITS_negative):\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(classify_sample)(LOGITS_positive, LOGITS_negative) for i in range(10))\n",
    "    print(mean(results),stdev(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.471332644628 0.003920557003008166\n"
     ]
    }
   ],
   "source": [
    "errors = classify_bagged(caucus_positive_scores, caucus_negative_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
