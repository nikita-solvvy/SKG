{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This script generates deep learning training data\n",
    "# Author: Nikita Mishra\n",
    "# run: python Deep_learning_training_data.py\n",
    "\n",
    "# TODO: remove repeated solutions and change start and end position accordingly\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import api_client\n",
    "from api_client import *\n",
    "import random\n",
    "\n",
    "\n",
    "#api_client = SourceFileLoader(\"module.name\", \"../api_client.py\").load_module()\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "SOLUTIONS_FILE = ''.join([DATA_DIR, 'solutions_org'])\n",
    "QUERIES_FILE   = ''.join([DATA_DIR, 'queries_org'])\n",
    "CAUCUSES_FILE  = ''.join([DATA_DIR, 'caucuses_org'])\n",
    "\n",
    "MAX_OUTPUT = 1000000\n",
    "\n",
    "MAX_LENGTH = 1000\n",
    "orgs_api = ApiClient('http://localhost:3002', '/v1/orgs')\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return\n",
    "\n",
    "def write(df, filename):\n",
    "    # takes a pandas dataframe and write as json file\n",
    "    df = df.to_json()\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(df, outfile)\n",
    "    print('Writing to json file {}'.format(filename))\n",
    "\n",
    "def read(filename):\n",
    "    # takes a json filename as input and loads a pandas dataframe\n",
    "    with open(filename) as data_file:\n",
    "        solutions = json.load(data_file)\n",
    "    print('Loading from file ', filename, '...')\n",
    "    return pd.read_json(solutions)\n",
    "\n",
    "# def load(org_id, option):  \n",
    "#     if option     == 'solutions':\n",
    "#         file_init, api_call = SOLUTIONS_FILE, api_client.solutions_api.get\n",
    "#         params    = {'org_id': org_id, 'is_canonical': 'true'}\n",
    "#     elif option   == 'queries':\n",
    "#         file_init, api_call = QUERIES_FILE, api_client.queries_api.get\n",
    "#         params    = {'org_id': org_id}\n",
    "#     elif option   == 'caucuses':\n",
    "#         file_init, api_call = CAUCUSES_FILE, api_client.caucuses_api.get\n",
    "#         params    ={'org_id':org_id}\n",
    "#     else:\n",
    "#         exit('Usage: load( org_id, <solutions, queries or caucuses>) ')\n",
    "         \n",
    "#     output_file = ''.join([file_init, str(org_id), '.json'])    \n",
    "#     if Path(output_file).exists():\n",
    "#         print('Downloaded-',option, 'file already exists!')\n",
    "#         final_output = read(output_file)        \n",
    "#     else:\n",
    "#         print('Downloading  and saving ',option)\n",
    "#         limit = 50\n",
    "#         offset = 0\n",
    "#         final_output = []\n",
    "#         while True:\n",
    "#             params['limit'], params['offset'] = limit, offset\n",
    "#             single_output = api_call(params=params)\n",
    "#             offset += len(single_output)\n",
    "#             final_output = final_output + single_output\n",
    "#             for sol in single_output:\n",
    "#                 if 'content' in sol.keys():\n",
    "#                     # Convert HTML tags\n",
    "#                     sol['content'] = BeautifulSoup(sol['content'], 'html.parser').get_text()               \n",
    "                \n",
    "#             if len(final_output)%1000==0:\n",
    "#                 print('Fetched %d single_output (total: %d)' % (len(single_output), len(final_output)))\n",
    "#             if len(single_output) == 0 or len(final_output) >= MAX_OUTPUT:\n",
    "#                 break\n",
    "        \n",
    "#         write(pd.DataFrame(final_output), output_file)\n",
    "#     return pd.DataFrame(final_output)\n",
    "\n",
    "def load(org_id, option):  \n",
    "    if option     == 'solutions':\n",
    "        file_init, api_call = SOLUTIONS_FILE, api_client.solutions_api.get\n",
    "        params    = {'org_id': org_id, 'is_canonical': 'true'}\n",
    "    elif option   == 'queries':\n",
    "        file_init, api_call = QUERIES_FILE, api_client.queries_api.get\n",
    "        params    = {'org_id': org_id}\n",
    "    elif option   == 'caucuses':\n",
    "        file_init, api_call = CAUCUSES_FILE, api_client.caucuses_api.get\n",
    "        params    ={'org_id':org_id}\n",
    "    else:\n",
    "        exit('Usage: load( org_id, <solutions, queries or caucuses>) ')\n",
    "         \n",
    "    output_file = ''.join([file_init, str(org_id), '.json'])    \n",
    "    if Path(output_file).exists():\n",
    "        print('Downloaded-',option, 'file already exists!')\n",
    "        final_output = read(output_file)        \n",
    "    else:\n",
    "        print('Downloading  and saving ',option)\n",
    "        limit = 50\n",
    "        offset = 0\n",
    "        final_output = []\n",
    "        while True:\n",
    "            params['limit'], params['offset'] = limit, offset\n",
    "            single_output = api_call(params=params).json()\n",
    "            offset += len(single_output)\n",
    "            final_output = final_output + single_output\n",
    "            for sol in single_output:\n",
    "                if 'content' in sol.keys():\n",
    "                    # Convert HTML tags\n",
    "                    sol['content'] = BeautifulSoup(sol['content'], 'html.parser').get_text()               \n",
    "                \n",
    "            if len(final_output)%1000==0:\n",
    "                print('Fetched %d single_output (total: %d)' % (len(single_output), len(final_output)))\n",
    "            if len(single_output) == 0 or len(final_output) >= MAX_OUTPUT:\n",
    "                break\n",
    "        \n",
    "        write(pd.DataFrame(final_output), output_file)\n",
    "    return pd.DataFrame(final_output)\n",
    "\n",
    "# def query_snippet_caucus(org_id):\n",
    "#     print('Running for org ->', org_id)\n",
    "#     queries   = load(org_id,'queries')\n",
    "#     solutions = load(org_id,'solutions')\n",
    "#     caucuses  = load(org_id,'caucuses')\n",
    "#     solutions = solutions.rename(columns = {'id':'solution_id','content':'solution_content'})\n",
    "#     queries   = queries.rename(columns = {'id':'query_id','content':'query_content'})    \n",
    "    \n",
    "#     caucuses  = caucuses.loc[caucuses['expert_upvotes'] > caucuses['expert_downvotes'] ]\n",
    "    \n",
    "#     caucuses  = pd.merge(caucuses, queries, on='query_id', how='inner')\n",
    "#     caucuses  = pd.merge(caucuses, solutions, on='solution_id', how='inner')\n",
    "    \n",
    "#     solutions = caucuses[['solution_content','solution_id']]\n",
    "#     queries   = caucuses[['query_content','query_id']]\n",
    "    \n",
    "#     # get unique solutions and get there start and end position in joined passage of unique solutions\n",
    "#     solutions_joined = ''.join( [row[0] for index, row in solutions.iterrows()])\n",
    "\n",
    "#     ## Start and End of the index are the start and end positions in solutions_joined\n",
    "#     for index, row in solutions.iterrows():\n",
    "#         n = len(row['solution_content'])\n",
    "#         if index==0:\n",
    "#             Start, End = [0], [n-1]\n",
    "#         else:\n",
    "#             Start.append(End[-1] + 1)\n",
    "#             End.append(Start[-1] + n -1 )\n",
    "            \n",
    "#     caucuses.insert(1, 'start', Start)\n",
    "#     caucuses.insert(1, 'end', End)    \n",
    "#     print('Summary: caucus_size = ',len(caucuses),', solution_size',len(solutions),',query_size',len(queries),'\\n done!')    \n",
    "#     return solutions_joined, caucuses[['org_id','query_id', 'solution_id','id','query_content', 'solution_content']]\n",
    "\n",
    "def query_snippet_caucus(org_id):\n",
    "    print('Running for org ->', org_id)\n",
    "    \n",
    "    output_file = ''.join([CAUCUSES_FILE,'_filtered', str(org_id), '.json'])\n",
    "    if not Path(output_file).exists():\n",
    "        \n",
    "        queries   = load(org_id,'queries')\n",
    "        solutions = load(org_id,'solutions')\n",
    "        caucuses  = load(org_id,'caucuses')\n",
    "        solutions = solutions.rename(columns = {'id':'solution_id','content':'solution_content'})\n",
    "        queries   = queries.rename(columns = {'id':'query_id','content':'query_content'})    \n",
    "\n",
    "        caucuses  = pd.merge(caucuses, queries, on='query_id', how='inner')\n",
    "        caucuses  = pd.merge(caucuses, solutions, on='solution_id', how='inner')\n",
    "        caucuses  = caucuses[['org_id','query_id', 'solution_id','id','query_content', 'solution_content', 'expert_upvotes','expert_downvotes' ]]\n",
    "\n",
    "        write(caucuses, output_file)\n",
    "        print('Summary: caucus_size = ',len(caucuses),', solution_size',len(solutions),',query_size',len(queries),'\\n done!')    \n",
    "    \n",
    "    else:\n",
    "        caucuses = read(output_file)\n",
    "        \n",
    "    return caucuses\n",
    "def query_solution_pairs(queries, solutions):\n",
    "    \n",
    "    output_file = ''.join([QUERIES_FILE,'_filtered', str(org_id), '.json'])\n",
    "    if not Path(output_file).exists():\n",
    "        queries_filtered = []\n",
    "        for index, row in queries.iterrows():\n",
    "            try:\n",
    "                if 'source_id' in row['sources'][0] and len(row['content']) < MAX_LENGTH:\n",
    "                    queries_filtered.append([row['sources'][0]['source_id'], row['id'], remove_special_char(row['content']) ])\n",
    "            except:\n",
    "                print(index,row)\n",
    "                exit(1)\n",
    "        queries_filtered   = pd.DataFrame( queries_filtered)\n",
    "        queries_filtered.columns = [ 'resource_id','query_id','query_content']\n",
    "        queries_filtered = queries_filtered.drop_duplicates(subset='resource_id')\n",
    "        write(queries_filtered, output_file)\n",
    "    else:\n",
    "        queries_filtered = read(output_file)\n",
    "    output_file = ''.join([SOLUTIONS_FILE,'_filtered', str(org_id), '.json'])    \n",
    "    if not Path(output_file).exists():\n",
    "        solutions_filtered = [[row['resource']['id'], row['id'],  remove_special_char(row['content'])] \n",
    "                    for index, row in solutions.iterrows() if len(row['content']) < MAX_LENGTH ]\n",
    "        solutions_filtered = pd.DataFrame(solutions_filtered)\n",
    "        solutions_filtered.columns = [ 'resource_id','solution_id','solution_content']\n",
    "        solutions_filtered = solutions_filtered.drop_duplicates(subset='resource_id')\n",
    "        write(solutions_filtered, output_file)\n",
    "    else:\n",
    "        solutions_filtered = read(output_file)        \n",
    "    qs_pairs  = queries_filtered.merge(solutions_filtered, on='resource_id', how='inner')\n",
    "    return qs_pairs\n",
    "\n",
    "\n",
    "def split(df,p_train,p_test,p_dev):\n",
    "    df = df.sample(frac=1, random_state=9001).reset_index(drop=True)\n",
    "    n = p_train+p_test+p_dev # just to make sure we have the split normalize\n",
    "    p_train,p_test,p_dev = p_train/n, p_test/n, p_dev/n    \n",
    "    train, validate, test = np.split(df.sample(frac=1), [int(p_train*len(df)), int((p_train+p_dev)*len(df))])\n",
    "    \n",
    "    dir_split_sample = ''.join([DATA_DIR, 'split_',str(org_id)])\n",
    "    create_dir(dir_split_sample )\n",
    "    \n",
    "    train_dir = ''.join([dir_split_sample ,'/train'])\n",
    "    test_dir  = ''.join([dir_split_sample ,'/test'])\n",
    "    dev_dir   = ''.join([dir_split_sample ,'/dev'])\n",
    "    \n",
    "    create_dir(train_dir)\n",
    "    create_dir(test_dir)\n",
    "    create_dir(dev_dir)\n",
    "\n",
    "    train['query_content'].to_csv(      ''.join([train_dir,'/sources.txt']), index=False,header=False)\n",
    "    train['solution_content'].to_csv(   ''.join([train_dir,'/targets.txt']), index=False,header=False)\n",
    "\n",
    "    validate['query_content'].to_csv(   ''.join([dev_dir,  '/sources.txt']), index=False,header=False)\n",
    "    validate['solution_content'].to_csv(''.join([dev_dir,  '/targets.txt']), index=False,header=False)\n",
    "\n",
    "    test['query_content'].to_csv(       ''.join([test_dir, '/sources.txt']), index=False,header=False)\n",
    "    test['solution_content'].to_csv(    ''.join([test_dir, '/targets.txt']), index=False,header=False)\n",
    "    return (train, validate, test)\n",
    "\n",
    "def split_sample(df,k,p_train,p_test,p_dev):\n",
    "    df = df.sample(frac=1, random_state=9001).reset_index(drop=True)\n",
    "    df = df.iloc[:k]\n",
    "    n = p_train+p_test+p_dev # just to make sure we have the split normalize\n",
    "    p_train,p_test,p_dev = p_train/n, p_test/n, p_dev/n    \n",
    "    train, validate, test = np.split(df.sample(frac=1), [int(p_train*len(df)), int((p_train+p_dev)*len(df))])\n",
    "    \n",
    "    dir_split_sample = ''.join([DATA_DIR, 'split_sample_',str(org_id)])\n",
    "    create_dir(dir_split_sample )\n",
    "    \n",
    "    train_dir = ''.join([dir_split_sample ,'/train'])\n",
    "    test_dir  = ''.join([dir_split_sample ,'/test'])\n",
    "    dev_dir   = ''.join([dir_split_sample ,'/dev'])\n",
    "    \n",
    "    create_dir(train_dir)\n",
    "    create_dir(test_dir)\n",
    "    create_dir(dev_dir)\n",
    "\n",
    "    train['query_content'].to_csv(      ''.join([train_dir,'/sources.txt']), index=False,header=False)\n",
    "    train['solution_content'].to_csv(   ''.join([train_dir,'/targets.txt']), index=False,header=False)\n",
    "\n",
    "    validate['query_content'].to_csv(   ''.join([dev_dir,  '/sources.txt']), index=False,header=False)\n",
    "    validate['solution_content'].to_csv(''.join([dev_dir,  '/targets.txt']), index=False,header=False)\n",
    "\n",
    "    test['query_content'].to_csv(       ''.join([test_dir, '/sources.txt']), index=False,header=False)\n",
    "    test['solution_content'].to_csv(    ''.join([test_dir, '/targets.txt']), index=False,header=False)\n",
    "    return (train, validate, test)\n",
    "# def remove_special_char(string):\n",
    "#     #return ''.join(e for e in string if e.isalnum())\n",
    "#     string = string.replace('\"', '')\n",
    "#     string = string.replace('\\'', '')\n",
    "#     string = string.replace('\\r', '')\n",
    "    \n",
    "#     return string.replace('\\n', '.')\n",
    "\n",
    "def remove_special_char(string):\n",
    "    strings_to_remove = ['Upwork Customer Support | . Blog: Building a Mobile App? A Prototype Saved Our Product and link to https://www.upwork.com/blog/2014/08/building-mobile-app-prototype-saved-product/.'  \n",
    "                         ,'\\\"','\\'','\\r','Hello', 'UNK', 'Thank you for contacting Upwork', 'Support &#124', \n",
    "                         'support.upwork.com', 'From the Upwork', 'Thanks for contacting Upwork','Upwork.com.',\n",
    "                         'Upwork Support Team','High Value Support',\n",
    "                         'Upwork Marketplace Quality Team'\n",
    "                        ]\n",
    "    \n",
    "    for item in strings_to_remove:\n",
    "        string = string.replace(item, '')\n",
    "    \n",
    "    return string.replace('\\n', '.')\n",
    "# def tokenize_and_copy(org_name):\n",
    "#     !mkdir /Users/nikita/nmt_data/$org_name\n",
    "#     !cp -r data/$org_name/* /Users/nikita/nmt_data/$org_name\n",
    "#     !bash /Users/nikita/Desktop/seq2seq/bin/data/tokenize.sh $org_name\n",
    "#     !scp -r /Users/nikita/nmt_data/$org_name deep_machine:/tmp/nmt_tutorial\n",
    "\n",
    "def tokenize_and_copy(org_id, org_name,typee):\n",
    "    split_dir  = ''.join([DATA_DIR, 'split_',typee,'_',str(org_id)])\n",
    "    nmt        = ''.join([ org_name,'_',typee])\n",
    "\n",
    "    !mkdir /Users/nikita/nmt_data/$nmt\n",
    "    !cp -r $split_dir/* /Users/nikita/nmt_data/$nmt\n",
    "    !bash /Users/nikita/Desktop/seq2seq/bin/data/tokenize.sh $nmt\n",
    "    !scp -r /Users/nikita/nmt_data/$nmt deep_machine:/tmp/nmt_tutorial\n",
    "\n",
    "def save_snippet(df):\n",
    "    \n",
    "    dir_split_sample = ''.join([DATA_DIR, 'split_snippet_',str(org_id)])\n",
    "    create_dir(dir_split_sample )\n",
    "    \n",
    "    train_dir = ''.join([dir_split_sample ,'/train'])\n",
    "    \n",
    "    create_dir(train_dir)\n",
    "    train = df\n",
    "    train['query_content'].to_csv(      ''.join([train_dir,'/sources.txt']), index=False,header=False)\n",
    "    train['solution_content'].to_csv(   ''.join([train_dir,'/targets.txt']), index=False,header=False)\n",
    "    train.to_csv(   ''.join([train_dir,'/df.txt']), index=False,header=False)\n",
    "    return \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     list_of_orgs = pd.read_csv('/tmp/list_of_active_orgs', delimiter=' ', header=None).iloc[0]\n",
    "#     for org_id in list_of_orgs:\n",
    "#         solutions_joined, caucuses = run(org_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_dir(DATA_DIR)\n",
    "           \n",
    "#org_id,org_name    = 7, 'alexa'\n",
    "org_id, org_name,typee   = 1, 'Upwork','snippet'\n",
    "\n",
    "        \n",
    "\n",
    "#org_name  = orgs_api.get(path=\"/%s\" % org_id)['name']\n",
    "queries   = load(org_id,'queries')\n",
    "solutions = load(org_id,'solutions')\n",
    "qs_pairs  = query_solution_pairs(queries, solutions)\n",
    "\n",
    "(train,validate,test) = split(qs_pairs,0.9,0.1,0.1)\n",
    "(train_sample,validate_sample,test_sample) = split_sample(qs_pairs,1000,0.9,0.1,0.1)\n",
    "\n",
    "tokenize_and_copy(org_id, org_name,'')\n",
    "tokenize_and_copy(org_id, org_name,'sample')\n",
    "\n",
    "# tokenize_and_copy(''.join(org_name))\n",
    "# tokenize_and_copy(''.join([org_name,'_sample']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for org -> 1\n",
      "Loading from file  data/caucuses_org_filtered1.json ...\n",
      "mkdir: /Users/nikita/nmt_data/Upwork_snippet: File exists\n",
      "Writing to /Users/nikita/nmt_data/Upwork_snippet. To change this, set the OUTPUT_DIR environment variable.\n",
      "Tokenizing /Users/nikita/nmt_data/Upwork_snippet/train/df.txt...\n",
      "Tokenizing /Users/nikita/nmt_data/Upwork_snippet/train/sources.txt...\n",
      "Tokenizing /Users/nikita/nmt_data/Upwork_snippet/train/targets.txt...\n",
      "Wrote /Users/nikita/nmt_data/Upwork_snippet/train/vocab.sources.tok\n",
      "Wrote /Users/nikita/nmt_data/Upwork_snippet/train/vocab.targets.tok\n",
      "All done.\n",
      "df.tok                                        100% 4984KB  24.5MB/s   00:00    \n",
      "df.txt                                        100% 4668KB  23.9MB/s   00:00    \n",
      "sources.tok                                   100% 1417KB  23.4MB/s   00:00    \n",
      "sources.txt                                   100% 1310KB  23.1MB/s   00:00    \n",
      "targets.tok                                   100% 2703KB  19.8MB/s   00:00    \n",
      "targets.txt                                   100% 2558KB  20.9MB/s   00:00    \n",
      "vocab.sources.tok                             100%   25KB   4.2MB/s   00:00    \n",
      "vocab.targets.tok                             100%   64KB   6.8MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "org_id, org_name,typee   = 1, 'Upwork','snippet'\n",
    "caucuses = query_snippet_caucus(1)\n",
    "save_snippet(caucuses) \n",
    "tokenize_and_copy(org_id, org_name,typee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_length = []\n",
    "s_length = []\n",
    "for index in range(len(qs_pairs)):\n",
    "    q_length.append(len(qs_pairs.iloc[index]['query_content']))\n",
    "    s_length.append(len(qs_pairs.iloc[index]['solution_content']))\n",
    "q_length = sorted(q_length)\n",
    "s_length = sorted(s_length)  \n",
    "print(np.median(q_length), np.median(s_length), max(q_length), max(s_length), min(q_length), min(s_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.plot(q_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "plt.plot(s_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded- queries file already exists!\n",
      "Loading from file  data/queries_org1.json ...\n",
      "Downloaded- solutions file already exists!\n",
      "Loading from file  data/solutions_org1.json ...\n"
     ]
    }
   ],
   "source": [
    "# Redo negative examples to have similar length distribution\n",
    "\n",
    "create_dir(DATA_DIR)\n",
    "           \n",
    "#org_id,org_name    = 7, 'alexa'\n",
    "org_id, org_name,typee   = 1, 'Upwork','snippet'\n",
    "\n",
    "        \n",
    "\n",
    "#org_name  = orgs_api.get(path=\"/%s\" % org_id)['name']\n",
    "queries   = load(org_id,'queries')\n",
    "solutions = load(org_id,'solutions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
