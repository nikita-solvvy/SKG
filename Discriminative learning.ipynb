{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import api_client\n",
    "from api_client import *\n",
    "import random\n",
    "import math\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "import math\n",
    "#api_client = SourceFileLoader(\"module.name\", \"../api_client.py\").load_module()\n",
    "\n",
    "DATA_DIR = '/home/solvvy/nmt_data/Upwork/'\n",
    "SOLUTIONS_FILE = ''.join([DATA_DIR, 'solutions_org'])\n",
    "QUERIES_FILE   = ''.join([DATA_DIR, 'queries_org'])\n",
    "CAUCUSES_FILE  = ''.join([DATA_DIR, 'caucuses_org'])\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return\n",
    "\n",
    "def read(filename):\n",
    "    print('Loading from file ', filename, '...')\n",
    "    with open(filename) as data_file:\n",
    "        return data_file.read().splitlines()\n",
    "    \n",
    "def read_padded(filename, MAX_LENGTH):\n",
    "    print('Loading from file ', filename, '...')\n",
    "    with open(filename) as data_file:\n",
    "        data= data_file.read().splitlines()\n",
    "        \n",
    "    return [pad(item, MAX_LENGTH) for item in data ]\n",
    "\n",
    "def pad(sentence,maxlength):\n",
    "    LEN = len(sentence.split(' '))\n",
    "    if LEN>=maxlength:\n",
    "        return sentence[:maxlength]\n",
    "    else:\n",
    "        return ' '.join([sentence]+['BUFFER_PAD']*(maxlength-LEN))\n",
    "    \n",
    "def length_distribution( solutions_caucus):\n",
    "    return [len(u.split(' ')) for u in solutions_caucus]\n",
    "\n",
    "\n",
    "def print_dist(x):\n",
    "    print('min:',min(x),'mean:',sum(x)/len(x),'max:',max(x))\n",
    "    \n",
    "def coocurrence_q_a(queries, solutions, vocab_words):\n",
    "    start_time = time.time()\n",
    "    coocurrence_vocab = []\n",
    "    for i, (query,solution) in  enumerate(zip(queries, solutions)):\n",
    "        gen = {'\\t'.join((x, y)) for x in set(query.split(' ')) \n",
    "                      for y in set(solution.split(' ')) \n",
    "                      if x in vocab_words and y in vocab_words}\n",
    "        coocurrence_vocab.append(Counter(gen))\n",
    "        if i%1000 == 0:\n",
    "            print('done with, i =', i,'in', time.time() - start_time,'seconds') \n",
    "    return coocurrence_vocab\n",
    "\n",
    "def intersection_q_a(queries, solutions, vocab_words):\n",
    "    # Identical to Chasm paper\n",
    "    start_time = time.time()\n",
    "    coocurrence_vocab = []\n",
    "    for i, (query,solution) in  enumerate(zip(queries, solutions)):\n",
    "        start_time1 = time.time()\n",
    "        qq, aa = Counter(query.split()), Counter(solution.split())\n",
    "        #print('1', time.time() - start_time1,'seconds') \n",
    "        qq_norm = math.sqrt( sum(map(lambda x:x*x, qq.values())))\n",
    "        aa_norm = math.sqrt( sum(map(lambda x:x*x, aa.values())))\n",
    "        gen = {key: (value*aa[key])/(qq_norm*aa_norm) for key,value in qq.items() if key in aa.keys()}\n",
    "\n",
    "        coocurrence_vocab.append(gen)\n",
    "        #print('2', time.time() - start_time2,'seconds') \n",
    "        if i%1000 == 0:\n",
    "            print('done with, i =', i,'in', time.time() - start_time,'seconds') \n",
    "    return coocurrence_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file  /home/solvvy/nmt_data/Upwork/train/sources.tok ...\n",
      "Loading from file  /home/solvvy/nmt_data/Upwork/train/targets.tok ...\n",
      "Loading from file  /home/solvvy/nmt_data/Upwork/train/vocab.sources.tok ...\n",
      "Loading from file  /home/solvvy/nmt_data/Upwork/train/vocab.targets.tok ...\n",
      "Loading from file  /home/solvvy/nmt_data/Upwork/caucus_positive/sources.tok ...\n",
      "Loading from file  /home/solvvy/nmt_data/Upwork/caucus_positive/targets.tok ...\n",
      "Loading from file  /home/solvvy/nmt_data/Upwork/caucus_negative/sources.tok ...\n",
      "Loading from file  /home/solvvy/nmt_data/Upwork/caucus_negative/targets.tok ...\n"
     ]
    }
   ],
   "source": [
    "queries = read(''.join([DATA_DIR,'train/sources.tok']))\n",
    "solutions = read(''.join([DATA_DIR,'train/targets.tok']))\n",
    "\n",
    "vocab_queries = read(''.join([DATA_DIR,'train/vocab.sources.tok']))\n",
    "vocab_solutions = read(''.join([DATA_DIR,'train/vocab.targets.tok']))\n",
    "vocab_queries_filtered = [row.split('\\t')[0] for row in vocab_queries]\n",
    "vocab_solutions_filtered = [row.split('\\t')[0] for row in vocab_solutions]\n",
    "\n",
    "# read caucus positive negative\n",
    "queries_caucus_positive = read(''.join([DATA_DIR, 'caucus_positive/sources.tok']))\n",
    "solutions_caucus_positive = read(''.join([DATA_DIR,'caucus_positive/targets.tok']))\n",
    "queries_caucus_negative = read(''.join([DATA_DIR,'caucus_negative/sources.tok']))\n",
    "solutions_caucus_negative = read(''.join([DATA_DIR, 'caucus_negative/targets.tok']))\n",
    "\n",
    "solutions_random = solutions.copy()\n",
    "random.shuffle(solutions_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length 208533 word at 20k ('Bhavna', 6)\n"
     ]
    }
   ],
   "source": [
    "# Loading vocabulary\n",
    "vocab = Counter()\n",
    "sentence = []\n",
    "for sentence in queries_caucus_positive:\n",
    "    vocab.update(sentence.split(' '))\n",
    "for sentence in solutions_caucus_positive:\n",
    "    vocab.update(sentence.split(' '))\n",
    "for sentence in solutions_caucus_negative:\n",
    "    vocab.update(sentence.split(' '))\n",
    "for sentence in queries:\n",
    "    vocab.update(sentence.split(' '))\n",
    "for sentence in solutions:\n",
    "    vocab.update(sentence.split(' '))\n",
    "\n",
    "print('Vocabulary length',len(vocab),'word at 20k',vocab.most_common(20000)[-1])  \n",
    "vocab_words = dict(vocab.most_common(10000)).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative model for Unigram co-occurance. It is very high dimensional and not sufficiently sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with, i = 0 in 0.004544258117675781 seconds\n",
      "done with, i = 1000 in 1.6310405731201172 seconds\n",
      "done with, i = 2000 in 3.202805280685425 seconds\n",
      "done with, i = 3000 in 4.754812002182007 seconds\n",
      "done with, i = 4000 in 6.2600417137146 seconds\n",
      "done with, i = 5000 in 7.875995874404907 seconds\n",
      "done with, i = 6000 in 9.383801698684692 seconds\n",
      "done with, i = 7000 in 11.019349813461304 seconds\n",
      "done with, i = 8000 in 12.713237047195435 seconds\n",
      "done with, i = 9000 in 14.25466537475586 seconds\n",
      "done with, i = 10000 in 15.923977136611938 seconds\n",
      "done with, i = 11000 in 17.409102201461792 seconds\n",
      "done with, i = 12000 in 18.99878716468811 seconds\n",
      "done with, i = 13000 in 20.734183073043823 seconds\n",
      "done with, i = 14000 in 22.370671033859253 seconds\n",
      "done with, i = 15000 in 23.963133335113525 seconds\n",
      "done with, i = 16000 in 25.53822112083435 seconds\n",
      "done with, i = 17000 in 27.05868148803711 seconds\n",
      "done with, i = 18000 in 28.65634560585022 seconds\n",
      "done with, i = 19000 in 30.15369176864624 seconds\n",
      "done with, i = 20000 in 31.723029851913452 seconds\n",
      "done with, i = 21000 in 33.33479380607605 seconds\n",
      "done with, i = 22000 in 35.2489218711853 seconds\n",
      "done with, i = 23000 in 36.839998960494995 seconds\n",
      "done with, i = 24000 in 38.33160662651062 seconds\n",
      "done with, i = 25000 in 39.92776393890381 seconds\n",
      "done with, i = 26000 in 41.47554612159729 seconds\n",
      "done with, i = 27000 in 42.97908973693848 seconds\n",
      "done with, i = 28000 in 44.47658085823059 seconds\n",
      "done with, i = 29000 in 46.01538419723511 seconds\n",
      "done with, i = 30000 in 47.6548228263855 seconds\n",
      "done with, i = 31000 in 49.157344579696655 seconds\n",
      "done with, i = 32000 in 50.66050434112549 seconds\n",
      "done with, i = 33000 in 52.17851448059082 seconds\n",
      "done with, i = 34000 in 53.74573731422424 seconds\n",
      "done with, i = 35000 in 55.21537804603577 seconds\n",
      "done with, i = 36000 in 56.67080640792847 seconds\n",
      "done with, i = 37000 in 58.37116575241089 seconds\n",
      "done with, i = 38000 in 59.916301250457764 seconds\n",
      "done with, i = 39000 in 61.53715467453003 seconds\n",
      "done with, i = 40000 in 63.18002367019653 seconds\n",
      "done with, i = 41000 in 64.72014927864075 seconds\n",
      "done with, i = 42000 in 66.26089882850647 seconds\n",
      "done with, i = 43000 in 67.86498355865479 seconds\n",
      "done with, i = 44000 in 69.40004992485046 seconds\n",
      "done with, i = 45000 in 71.04874300956726 seconds\n",
      "done with, i = 46000 in 72.55114889144897 seconds\n",
      "done with, i = 47000 in 74.09618282318115 seconds\n",
      "done with, i = 48000 in 75.68860626220703 seconds\n",
      "done with, i = 49000 in 77.23865413665771 seconds\n",
      "done with, i = 50000 in 78.75353121757507 seconds\n",
      "done with, i = 51000 in 80.25007581710815 seconds\n",
      "done with, i = 52000 in 81.7391767501831 seconds\n",
      "done with, i = 53000 in 83.3160502910614 seconds\n",
      "done with, i = 54000 in 84.79750871658325 seconds\n",
      "done with, i = 55000 in 86.38748669624329 seconds\n",
      "done with, i = 56000 in 87.89365196228027 seconds\n",
      "done with, i = 57000 in 89.38869047164917 seconds\n",
      "done with, i = 58000 in 90.93719816207886 seconds\n",
      "done with, i = 59000 in 92.47060990333557 seconds\n",
      "done with, i = 60000 in 94.02493786811829 seconds\n",
      "done with, i = 61000 in 95.63291788101196 seconds\n",
      "done with, i = 62000 in 97.15150189399719 seconds\n",
      "done with, i = 63000 in 98.70707702636719 seconds\n",
      "done with, i = 64000 in 100.55752491950989 seconds\n",
      "done with, i = 65000 in 102.05153560638428 seconds\n",
      "done with, i = 66000 in 103.57554221153259 seconds\n",
      "done with, i = 67000 in 105.11933660507202 seconds\n",
      "done with, i = 68000 in 106.67737913131714 seconds\n",
      "done with, i = 69000 in 108.33083581924438 seconds\n",
      "done with, i = 70000 in 109.8504581451416 seconds\n",
      "done with, i = 71000 in 111.37191605567932 seconds\n",
      "done with, i = 72000 in 112.98076605796814 seconds\n",
      "done with, i = 73000 in 114.55971360206604 seconds\n",
      "done with, i = 74000 in 116.09427285194397 seconds\n",
      "done with, i = 75000 in 117.6792962551117 seconds\n",
      "done with, i = 76000 in 119.20964980125427 seconds\n",
      "done with, i = 77000 in 120.72644376754761 seconds\n",
      "done with, i = 78000 in 122.46411108970642 seconds\n",
      "done with, i = 79000 in 124.05185055732727 seconds\n",
      "done with, i = 80000 in 125.78331685066223 seconds\n",
      "done with, i = 81000 in 127.32400560379028 seconds\n",
      "done with, i = 82000 in 128.92965841293335 seconds\n",
      "done with, i = 83000 in 130.48161149024963 seconds\n",
      "done with, i = 84000 in 132.00049662590027 seconds\n",
      "done with, i = 85000 in 133.65881180763245 seconds\n",
      "done with, i = 86000 in 135.1916310787201 seconds\n",
      "done with, i = 0 in 0.002101421356201172 seconds\n",
      "done with, i = 1000 in 1.593139886856079 seconds\n",
      "done with, i = 2000 in 3.2951769828796387 seconds\n",
      "done with, i = 3000 in 4.896239995956421 seconds\n",
      "done with, i = 4000 in 6.378611087799072 seconds\n",
      "done with, i = 5000 in 7.917561054229736 seconds\n",
      "done with, i = 6000 in 9.39769721031189 seconds\n",
      "done with, i = 7000 in 10.88236689567566 seconds\n",
      "done with, i = 8000 in 12.361440420150757 seconds\n",
      "done with, i = 9000 in 13.916759252548218 seconds\n",
      "done with, i = 10000 in 15.487010478973389 seconds\n",
      "done with, i = 11000 in 17.07156777381897 seconds\n",
      "done with, i = 12000 in 18.693794012069702 seconds\n",
      "done with, i = 13000 in 20.23910093307495 seconds\n",
      "done with, i = 14000 in 21.74982976913452 seconds\n",
      "done with, i = 15000 in 23.293033123016357 seconds\n",
      "done with, i = 16000 in 24.867712259292603 seconds\n",
      "done with, i = 17000 in 26.36858630180359 seconds\n",
      "done with, i = 18000 in 27.945250272750854 seconds\n",
      "done with, i = 19000 in 29.567689657211304 seconds\n",
      "done with, i = 20000 in 31.111554622650146 seconds\n",
      "done with, i = 21000 in 32.75885200500488 seconds\n",
      "done with, i = 22000 in 34.28038024902344 seconds\n",
      "done with, i = 23000 in 35.79772615432739 seconds\n",
      "done with, i = 24000 in 37.37779355049133 seconds\n",
      "done with, i = 25000 in 38.90662169456482 seconds\n",
      "done with, i = 26000 in 40.65193438529968 seconds\n",
      "done with, i = 27000 in 42.11478114128113 seconds\n",
      "done with, i = 28000 in 45.25778341293335 seconds\n",
      "done with, i = 29000 in 46.792808532714844 seconds\n",
      "done with, i = 30000 in 48.36594367027283 seconds\n",
      "done with, i = 31000 in 49.85762619972229 seconds\n",
      "done with, i = 32000 in 51.430827617645264 seconds\n",
      "done with, i = 33000 in 52.97869563102722 seconds\n",
      "done with, i = 34000 in 54.95015072822571 seconds\n",
      "done with, i = 35000 in 56.91071009635925 seconds\n",
      "done with, i = 36000 in 60.37633776664734 seconds\n",
      "done with, i = 37000 in 63.28565716743469 seconds\n",
      "done with, i = 38000 in 64.84182143211365 seconds\n",
      "done with, i = 39000 in 66.35092616081238 seconds\n",
      "done with, i = 40000 in 67.91598606109619 seconds\n",
      "done with, i = 41000 in 69.42410230636597 seconds\n",
      "done with, i = 42000 in 71.87431192398071 seconds\n",
      "done with, i = 43000 in 78.16178631782532 seconds\n",
      "done with, i = 44000 in 79.75933790206909 seconds\n",
      "done with, i = 45000 in 81.41903376579285 seconds\n",
      "done with, i = 46000 in 82.92204976081848 seconds\n",
      "done with, i = 47000 in 84.34484934806824 seconds\n",
      "done with, i = 48000 in 85.9453649520874 seconds\n",
      "done with, i = 49000 in 87.4838182926178 seconds\n",
      "done with, i = 50000 in 92.93859601020813 seconds\n",
      "done with, i = 51000 in 94.85469841957092 seconds\n",
      "done with, i = 52000 in 96.44201111793518 seconds\n",
      "done with, i = 53000 in 98.30910396575928 seconds\n",
      "done with, i = 54000 in 99.83023595809937 seconds\n",
      "done with, i = 55000 in 104.43365621566772 seconds\n",
      "done with, i = 56000 in 108.43973755836487 seconds\n",
      "done with, i = 57000 in 109.95457935333252 seconds\n",
      "done with, i = 58000 in 111.44744968414307 seconds\n",
      "done with, i = 59000 in 113.01700901985168 seconds\n",
      "done with, i = 60000 in 114.77066135406494 seconds\n",
      "done with, i = 61000 in 116.29099583625793 seconds\n",
      "done with, i = 62000 in 122.91782116889954 seconds\n",
      "done with, i = 63000 in 124.82080674171448 seconds\n",
      "done with, i = 64000 in 126.50951480865479 seconds\n",
      "done with, i = 65000 in 128.0405149459839 seconds\n",
      "done with, i = 66000 in 129.57585263252258 seconds\n",
      "done with, i = 67000 in 131.11331272125244 seconds\n",
      "done with, i = 68000 in 136.023512840271 seconds\n",
      "done with, i = 69000 in 139.42640089988708 seconds\n",
      "done with, i = 70000 in 141.23638343811035 seconds\n",
      "done with, i = 71000 in 143.10390448570251 seconds\n",
      "done with, i = 72000 in 144.65411043167114 seconds\n",
      "done with, i = 73000 in 146.1989631652832 seconds\n",
      "done with, i = 74000 in 151.85895538330078 seconds\n",
      "done with, i = 75000 in 154.33344626426697 seconds\n",
      "done with, i = 76000 in 156.19824814796448 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with, i = 77000 in 157.8370521068573 seconds\n",
      "done with, i = 78000 in 159.45457220077515 seconds\n",
      "done with, i = 79000 in 160.97728753089905 seconds\n",
      "done with, i = 80000 in 167.91716122627258 seconds\n",
      "done with, i = 81000 in 170.44360065460205 seconds\n",
      "done with, i = 82000 in 171.99191451072693 seconds\n",
      "done with, i = 83000 in 173.71650624275208 seconds\n",
      "done with, i = 84000 in 175.33914995193481 seconds\n",
      "done with, i = 85000 in 177.03246784210205 seconds\n",
      "done with, i = 86000 in 178.5847418308258 seconds\n",
      "done with, i = 0 in 0.002121448516845703 seconds\n",
      "done with, i = 1000 in 1.0667028427124023 seconds\n",
      "done with, i = 0 in 0.0009911060333251953 seconds\n",
      "done with, i = 1000 in 0.7435486316680908 seconds\n"
     ]
    }
   ],
   "source": [
    "coocurrence_vocab_positive = coocurrence_q_a(queries, solutions, vocab_words)\n",
    "coocurrence_vocab_negative = coocurrence_q_a(queries, solutions_random, vocab_words)\n",
    "coocurrence_vocab_caucus_positive = coocurrence_q_a(queries_caucus_positive, \n",
    "                                                    solutions_caucus_positive, \n",
    "                                                    vocab_words)\n",
    "coocurrence_vocab_caucus_negative = coocurrence_q_a(queries_caucus_negative, \n",
    "                                                    solutions_caucus_negative, \n",
    "                                                    vocab_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "h = FeatureHasher(n_features=100000)\n",
    "f = h.transform(coocurrence_vocab_positive + \n",
    "                coocurrence_vocab_negative + \n",
    "                coocurrence_vocab_caucus_positive + \n",
    "                coocurrence_vocab_caucus_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_pos = len(coocurrence_vocab_positive)\n",
    "len_neg = len_pos\n",
    "len_caucus_pos = len( coocurrence_vocab_caucus_positive)\n",
    "len_caucus_neg = len( coocurrence_vocab_caucus_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = f\n",
    "y = [1]*len_pos + [-1]*len_neg + [1]*len_caucus_pos + [-1]*len_caucus_neg\n",
    "X_train, y_train = X[:(len_pos+len_neg),:], [1]*len_pos + [-1]*len_neg\n",
    "X_test, y_test = X[(len_pos+len_neg):,:], [1]*len_caucus_pos + [-1]*len_caucus_neg\n",
    "print(X_train.shape, X_test.shape,  len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = PassiveAggressiveClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chasm: http://www.cs.cmu.edu/~aberger/pdf/chasm.pdf Score function is based on cosine similarity in question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with, i = 0 in 0.00017142295837402344 seconds\n",
      "done with, i = 1000 in 0.07414078712463379 seconds\n",
      "done with, i = 2000 in 0.1353142261505127 seconds\n",
      "done with, i = 3000 in 0.19367671012878418 seconds\n",
      "done with, i = 4000 in 0.2519543170928955 seconds\n",
      "done with, i = 5000 in 0.3118431568145752 seconds\n",
      "done with, i = 6000 in 0.3706398010253906 seconds\n",
      "done with, i = 7000 in 0.4298279285430908 seconds\n",
      "done with, i = 8000 in 0.5009832382202148 seconds\n",
      "done with, i = 9000 in 0.5661888122558594 seconds\n",
      "done with, i = 10000 in 0.6257038116455078 seconds\n",
      "done with, i = 11000 in 0.6843912601470947 seconds\n",
      "done with, i = 12000 in 0.742711067199707 seconds\n",
      "done with, i = 13000 in 0.8039770126342773 seconds\n",
      "done with, i = 14000 in 0.8610172271728516 seconds\n",
      "done with, i = 15000 in 0.9159691333770752 seconds\n",
      "done with, i = 16000 in 0.9709913730621338 seconds\n",
      "done with, i = 17000 in 1.0250880718231201 seconds\n",
      "done with, i = 18000 in 1.0804426670074463 seconds\n",
      "done with, i = 19000 in 1.1338083744049072 seconds\n",
      "done with, i = 20000 in 1.188342809677124 seconds\n",
      "done with, i = 21000 in 1.2418832778930664 seconds\n",
      "done with, i = 22000 in 1.3102257251739502 seconds\n",
      "done with, i = 23000 in 1.363708257675171 seconds\n",
      "done with, i = 24000 in 1.4177753925323486 seconds\n",
      "done with, i = 25000 in 1.470569372177124 seconds\n",
      "done with, i = 26000 in 1.525242805480957 seconds\n",
      "done with, i = 27000 in 1.578505039215088 seconds\n",
      "done with, i = 28000 in 1.6464786529541016 seconds\n",
      "done with, i = 29000 in 1.7330105304718018 seconds\n",
      "done with, i = 30000 in 1.7967174053192139 seconds\n",
      "done with, i = 31000 in 1.849867820739746 seconds\n",
      "done with, i = 32000 in 1.9040417671203613 seconds\n",
      "done with, i = 33000 in 1.9579548835754395 seconds\n",
      "done with, i = 34000 in 2.012281656265259 seconds\n",
      "done with, i = 35000 in 2.065446615219116 seconds\n",
      "done with, i = 36000 in 2.118678569793701 seconds\n",
      "done with, i = 37000 in 2.1740496158599854 seconds\n",
      "done with, i = 38000 in 2.2287964820861816 seconds\n",
      "done with, i = 39000 in 2.2818939685821533 seconds\n",
      "done with, i = 40000 in 2.336091995239258 seconds\n",
      "done with, i = 41000 in 2.3902275562286377 seconds\n",
      "done with, i = 42000 in 2.445404529571533 seconds\n",
      "done with, i = 43000 in 2.53287410736084 seconds\n",
      "done with, i = 44000 in 2.619001865386963 seconds\n",
      "done with, i = 45000 in 2.705103635787964 seconds\n",
      "done with, i = 46000 in 2.7906181812286377 seconds\n",
      "done with, i = 47000 in 2.863128662109375 seconds\n",
      "done with, i = 48000 in 2.917585849761963 seconds\n",
      "done with, i = 49000 in 2.9714324474334717 seconds\n",
      "done with, i = 50000 in 3.024792194366455 seconds\n",
      "done with, i = 51000 in 3.0781211853027344 seconds\n",
      "done with, i = 52000 in 3.131387710571289 seconds\n",
      "done with, i = 53000 in 3.1859021186828613 seconds\n",
      "done with, i = 54000 in 3.2394561767578125 seconds\n",
      "done with, i = 55000 in 3.293046474456787 seconds\n",
      "done with, i = 56000 in 3.3470582962036133 seconds\n",
      "done with, i = 57000 in 3.4005117416381836 seconds\n",
      "done with, i = 58000 in 3.454200029373169 seconds\n",
      "done with, i = 59000 in 3.5082437992095947 seconds\n",
      "done with, i = 60000 in 3.5828857421875 seconds\n",
      "done with, i = 61000 in 3.6692168712615967 seconds\n",
      "done with, i = 62000 in 3.7282118797302246 seconds\n",
      "done with, i = 63000 in 3.783015489578247 seconds\n",
      "done with, i = 64000 in 3.8365986347198486 seconds\n",
      "done with, i = 65000 in 3.8900253772735596 seconds\n",
      "done with, i = 66000 in 3.9435980319976807 seconds\n",
      "done with, i = 67000 in 4.024707317352295 seconds\n",
      "done with, i = 68000 in 4.111394643783569 seconds\n",
      "done with, i = 69000 in 4.171606779098511 seconds\n",
      "done with, i = 70000 in 4.225323915481567 seconds\n",
      "done with, i = 71000 in 4.278700590133667 seconds\n",
      "done with, i = 72000 in 4.334349632263184 seconds\n",
      "done with, i = 73000 in 4.4213643074035645 seconds\n",
      "done with, i = 74000 in 4.497189044952393 seconds\n",
      "done with, i = 75000 in 4.551866769790649 seconds\n",
      "done with, i = 76000 in 4.605974435806274 seconds\n",
      "done with, i = 77000 in 4.659717082977295 seconds\n",
      "done with, i = 78000 in 4.713838338851929 seconds\n",
      "done with, i = 79000 in 4.7681496143341064 seconds\n",
      "done with, i = 80000 in 4.822237014770508 seconds\n",
      "done with, i = 81000 in 4.87730860710144 seconds\n",
      "done with, i = 82000 in 4.9314703941345215 seconds\n",
      "done with, i = 83000 in 4.98574423789978 seconds\n",
      "done with, i = 84000 in 5.043537139892578 seconds\n",
      "done with, i = 85000 in 5.128282308578491 seconds\n",
      "done with, i = 86000 in 5.214679718017578 seconds\n",
      "done with, i = 0 in 8.130073547363281e-05 seconds\n",
      "done with, i = 1000 in 0.0597381591796875 seconds\n",
      "done with, i = 2000 in 0.11410641670227051 seconds\n",
      "done with, i = 3000 in 0.16662001609802246 seconds\n",
      "done with, i = 4000 in 0.21968340873718262 seconds\n",
      "done with, i = 5000 in 0.2993762493133545 seconds\n",
      "done with, i = 6000 in 0.3828461170196533 seconds\n",
      "done with, i = 7000 in 0.4363565444946289 seconds\n",
      "done with, i = 8000 in 0.4892430305480957 seconds\n",
      "done with, i = 9000 in 0.5428431034088135 seconds\n",
      "done with, i = 10000 in 0.5966713428497314 seconds\n",
      "done with, i = 11000 in 0.6498594284057617 seconds\n",
      "done with, i = 12000 in 0.703629732131958 seconds\n",
      "done with, i = 13000 in 0.757551908493042 seconds\n",
      "done with, i = 14000 in 0.8102397918701172 seconds\n",
      "done with, i = 15000 in 0.8645644187927246 seconds\n",
      "done with, i = 16000 in 0.9191346168518066 seconds\n",
      "done with, i = 17000 in 0.9727253913879395 seconds\n",
      "done with, i = 18000 in 1.0264992713928223 seconds\n",
      "done with, i = 19000 in 1.0799319744110107 seconds\n",
      "done with, i = 20000 in 1.1328651905059814 seconds\n",
      "done with, i = 21000 in 1.186352252960205 seconds\n",
      "done with, i = 22000 in 1.239673137664795 seconds\n",
      "done with, i = 23000 in 1.292752981185913 seconds\n",
      "done with, i = 24000 in 1.3462488651275635 seconds\n",
      "done with, i = 25000 in 1.3988595008850098 seconds\n",
      "done with, i = 26000 in 1.451838731765747 seconds\n",
      "done with, i = 27000 in 1.5055434703826904 seconds\n",
      "done with, i = 28000 in 1.5596230030059814 seconds\n",
      "done with, i = 29000 in 1.613795518875122 seconds\n",
      "done with, i = 30000 in 1.6669659614562988 seconds\n",
      "done with, i = 31000 in 1.725921630859375 seconds\n",
      "done with, i = 32000 in 1.7944579124450684 seconds\n",
      "done with, i = 33000 in 1.8475472927093506 seconds\n",
      "done with, i = 34000 in 1.9003651142120361 seconds\n",
      "done with, i = 35000 in 1.9524378776550293 seconds\n",
      "done with, i = 36000 in 2.0047149658203125 seconds\n",
      "done with, i = 37000 in 2.058619260787964 seconds\n",
      "done with, i = 38000 in 2.112489700317383 seconds\n",
      "done with, i = 39000 in 2.1658313274383545 seconds\n",
      "done with, i = 40000 in 2.2199583053588867 seconds\n",
      "done with, i = 41000 in 2.2735328674316406 seconds\n",
      "done with, i = 42000 in 2.3272366523742676 seconds\n",
      "done with, i = 43000 in 2.3855364322662354 seconds\n",
      "done with, i = 44000 in 2.4716622829437256 seconds\n",
      "done with, i = 45000 in 2.55784273147583 seconds\n",
      "done with, i = 46000 in 2.6431055068969727 seconds\n",
      "done with, i = 47000 in 2.727865219116211 seconds\n",
      "done with, i = 48000 in 2.814802646636963 seconds\n",
      "done with, i = 49000 in 2.871248722076416 seconds\n",
      "done with, i = 50000 in 2.9242310523986816 seconds\n",
      "done with, i = 51000 in 2.9777672290802 seconds\n",
      "done with, i = 52000 in 3.031207799911499 seconds\n",
      "done with, i = 53000 in 3.086358070373535 seconds\n",
      "done with, i = 54000 in 3.1388914585113525 seconds\n",
      "done with, i = 55000 in 3.1929779052734375 seconds\n",
      "done with, i = 56000 in 3.2460720539093018 seconds\n",
      "done with, i = 57000 in 3.2993717193603516 seconds\n",
      "done with, i = 58000 in 3.3524913787841797 seconds\n",
      "done with, i = 59000 in 3.406372308731079 seconds\n",
      "done with, i = 60000 in 3.460545778274536 seconds\n",
      "done with, i = 61000 in 3.5144262313842773 seconds\n",
      "done with, i = 62000 in 3.5686047077178955 seconds\n",
      "done with, i = 63000 in 3.622821807861328 seconds\n",
      "done with, i = 64000 in 3.676419496536255 seconds\n",
      "done with, i = 65000 in 3.729910135269165 seconds\n",
      "done with, i = 66000 in 3.783008575439453 seconds\n",
      "done with, i = 67000 in 3.8367903232574463 seconds\n",
      "done with, i = 68000 in 3.8904550075531006 seconds\n",
      "done with, i = 69000 in 3.9452872276306152 seconds\n",
      "done with, i = 70000 in 3.998831033706665 seconds\n",
      "done with, i = 71000 in 4.051864147186279 seconds\n",
      "done with, i = 72000 in 4.1049277782440186 seconds\n",
      "done with, i = 73000 in 4.158404588699341 seconds\n",
      "done with, i = 74000 in 4.212085247039795 seconds\n",
      "done with, i = 75000 in 4.266178131103516 seconds\n",
      "done with, i = 76000 in 4.319691181182861 seconds\n",
      "done with, i = 77000 in 4.37347149848938 seconds\n",
      "done with, i = 78000 in 4.4269866943359375 seconds\n",
      "done with, i = 79000 in 4.481137275695801 seconds\n",
      "done with, i = 80000 in 4.5356080532073975 seconds\n",
      "done with, i = 81000 in 4.588792324066162 seconds\n",
      "done with, i = 82000 in 4.642053127288818 seconds\n",
      "done with, i = 83000 in 4.696072340011597 seconds\n",
      "done with, i = 84000 in 4.749162435531616 seconds\n",
      "done with, i = 85000 in 4.801578044891357 seconds\n",
      "done with, i = 86000 in 4.884108543395996 seconds\n",
      "done with, i = 0 in 0.00020647048950195312 seconds\n",
      "done with, i = 1000 in 0.06259799003601074 seconds\n",
      "done with, i = 0 in 8.416175842285156e-05 seconds\n",
      "done with, i = 1000 in 0.062413692474365234 seconds\n"
     ]
    }
   ],
   "source": [
    "vocab_words = dict(vocab.most_common(10000)).keys()\n",
    "intersection_vocab_positive = intersection_q_a(queries, solutions, vocab_words)\n",
    "intersection_vocab_negative = intersection_q_a(queries, solutions_random, vocab_words)\n",
    "intersection_vocab_caucus_positive = intersection_q_a(queries_caucus_positive, \n",
    "                                                    solutions_caucus_positive, \n",
    "                                                    vocab_words)\n",
    "intersection_vocab_caucus_negative = intersection_q_a(queries_caucus_negative, \n",
    "                                                    solutions_caucus_negative, \n",
    "                                                    vocab_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "h = FeatureHasher(n_features=10000)\n",
    "f = h.transform(intersection_vocab_positive + \n",
    "                intersection_vocab_negative + \n",
    "                intersection_vocab_caucus_positive + \n",
    "                intersection_vocab_caucus_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172090, 10000) (2764, 10000) 172090 2764\n"
     ]
    }
   ],
   "source": [
    "X = f\n",
    "y = [1]*len_pos + [-1]*len_neg + [1]*len_caucus_pos + [-1]*len_caucus_neg\n",
    "X_train, y_train = X[:(len_pos+len_neg),:], [1]*len_pos + [-1]*len_neg\n",
    "X_test, y_test = X[(len_pos+len_neg):,:], [1]*len_caucus_pos + [-1]*len_caucus_neg\n",
    "print(X_train.shape, X_test.shape,  len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706584659913\n"
     ]
    }
   ],
   "source": [
    "clf = PassiveAggressiveClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675470332851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(max_depth=50, random_state=0)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "pred = clf_rf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.552821997106\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172090, 10000) (2764, 10000) 172090 2764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "h = FeatureHasher(n_features=10000, non_negative=True)\n",
    "f = h.transform(intersection_vocab_positive + \n",
    "                intersection_vocab_negative + \n",
    "                intersection_vocab_caucus_positive + \n",
    "                intersection_vocab_caucus_negative)\n",
    "X = f\n",
    "y = [1]*len_pos + [-1]*len_neg + [1]*len_caucus_pos + [-1]*len_caucus_neg\n",
    "X_train, y_train = X[:(len_pos+len_neg),:], [1]*len_pos + [-1]*len_neg\n",
    "X_test, y_test = X[(len_pos+len_neg):,:], [1]*len_caucus_pos + [-1]*len_caucus_neg\n",
    "print(X_train.shape, X_test.shape,  len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try all classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--report\",\n",
    "              action=\"store_true\", dest=\"print_report\",\n",
    "              help=\"Print a detailed classification report.\")\n",
    "op.add_option(\"--chi2_select\",\n",
    "              action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
    "              help=\"Select some number of features using a chi-squared test\")\n",
    "op.add_option(\"--confusion_matrix\",\n",
    "              action=\"store_true\", dest=\"print_cm\",\n",
    "              help=\"Print the confusion matrix.\")\n",
    "op.add_option(\"--top10\",\n",
    "              action=\"store_true\", dest=\"print_top10\",\n",
    "              help=\"Print ten most discriminative terms per class\"\n",
    "                   \" for every classifier.\")\n",
    "op.add_option(\"--all_categories\",\n",
    "              action=\"store_true\", dest=\"all_categories\",\n",
    "              help=\"Whether to use all categories or not.\")\n",
    "op.add_option(\"--use_hashing\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Use a hashing vectorizer.\")\n",
    "op.add_option(\"--n_features\",\n",
    "              action=\"store\", type=int, default=2 ** 16,\n",
    "              help=\"n_features when using the hashing vectorizer.\")\n",
    "op.add_option(\"--filtered\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Remove newsgroup information that is easily overfit: \"\n",
    "                   \"headers, signatures, and quoting.\")\n",
    "(opts, args) = op.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='lsqr',\n",
      "        tol=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solvvy/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:311: UserWarning: In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "  warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.368s\n",
      "test time:  0.000s\n",
      "accuracy:   0.716\n",
      "dimensionality: 10000\n",
      "density: 0.699000\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=50, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "train time: 0.972s\n",
      "test time:  0.001s\n",
      "accuracy:   0.727\n",
      "dimensionality: 10000\n",
      "density: 0.658900\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, class_weight=None, fit_intercept=True,\n",
      "              loss='hinge', n_iter=50, n_jobs=1, random_state=None,\n",
      "              shuffle=True, verbose=0, warm_start=False)\n",
      "train time: 1.263s\n",
      "test time:  0.001s\n",
      "accuracy:   0.738\n",
      "dimensionality: 10000\n",
      "density: 0.684600\n",
      "\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.025s\n",
      "test time:  12.785s\n",
      "accuracy:   0.618\n",
      "\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "train time: 485.589s\n",
      "test time:  0.240s\n",
      "accuracy:   0.711\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "train time: 1.119s\n",
      "test time:  0.001s\n",
      "accuracy:   0.728\n",
      "dimensionality: 10000\n",
      "density: 0.700200\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 1.098s\n",
      "test time:  0.000s\n",
      "accuracy:   0.549\n",
      "dimensionality: 10000\n",
      "density: 0.698600\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "train time: 0.587s\n",
      "test time:  0.001s\n",
      "accuracy:   0.730\n",
      "dimensionality: 10000\n",
      "density: 0.034900\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 1.543s\n",
      "test time:  0.001s\n",
      "accuracy:   0.589\n",
      "dimensionality: 10000\n",
      "density: 0.003700\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 1.947s\n",
      "test time:  0.001s\n",
      "accuracy:   0.548\n",
      "dimensionality: 10000\n",
      "density: 0.014700\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "train time: 0.086s\n",
      "test time:  0.001s\n",
      "accuracy:   0.573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    if opts.print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if opts.print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\n",
    "                                       tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.043s\n",
      "test time:  0.000s\n",
      "accuracy:   0.745\n",
      "dimensionality: 10000\n",
      "density: 1.000000\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.050s\n",
      "test time:  0.002s\n",
      "accuracy:   0.696\n",
      "dimensionality: 10000\n",
      "density: 1.000000\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0),\n",
      "        prefit=False, thresho...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "train time: 1.638s\n",
      "test time:  0.002s\n",
      "accuracy:   0.724\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAI1CAYAAAB8GvSWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31368a3208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                  tol=1e-3))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))])))\n",
    "\n",
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
